import streamlit as st
import httpx
import json
import os
import re

# ============================================================
# é…ç½® - ä½¿ç”¨ OpenRouter API
# ============================================================
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
MODEL_NAME = "stepfun/step-3.5-flash:free"
HF_CONFIG_URL = "https://huggingface.co/stepfun-ai/Step-3.5-Flash/raw/main/config.json"
STEPFUN_LOGO_URL = "https://huggingface.co/stepfun-ai/Step-3.5-Flash/resolve/main/stepfun.svg"
STEPFUN_LOGO_PATH = "/tmp/stepfun_logo.svg"


def download_logo():
    """ä¸‹è½½ StepFun logo åˆ°æœ¬åœ°"""
    try:
        response = httpx.get(STEPFUN_LOGO_URL, timeout=10.0, follow_redirects=True)
        if response.status_code == 200:
            with open(STEPFUN_LOGO_PATH, "wb") as f:
                f.write(response.content)
            return True
    except Exception:
        pass
    return False


def get_assistant_avatar():
    """è·å–åŠ©æ‰‹å¤´åƒï¼Œä¼˜å…ˆä½¿ç”¨ä¸‹è½½çš„ logoï¼Œå¤±è´¥åˆ™ç”¨ emoji"""
    if os.path.exists(STEPFUN_LOGO_PATH):
        return STEPFUN_LOGO_PATH
    return "ğŸš€"


# å¯åŠ¨æ—¶ä¸‹è½½ logo
download_logo()

st.set_page_config(
    page_title="Step-3.5-Flash",
    page_icon="ğŸš€",
    layout="centered",
)

# ç®€åŒ–æ ·å¼ - åªå®šä¹‰æ€è€ƒåŒºåŸŸ
st.markdown("""
<style>
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}

.thinking-container {
    background: #f8fafc;
    border: 1px solid #e2e8f0;
    border-radius: 8px;
    padding: 10px 14px;
    margin-bottom: 10px;
    max-height: 150px;
    overflow-y: auto;
    font-size: 13px;
    line-height: 1.5;
    color: #64748b;
}
.thinking-container::-webkit-scrollbar {
    width: 4px;
}
.thinking-container::-webkit-scrollbar-thumb {
    background: #cbd5e1;
    border-radius: 2px;
}
.thinking-label {
    font-size: 12px;
    color: #94a3b8;
    margin-bottom: 4px;
}
</style>
""", unsafe_allow_html=True)


@st.cache_data(ttl=3600)
def fetch_model_config():
    try:
        response = httpx.get(HF_CONFIG_URL, timeout=10.0)
        if response.status_code == 200:
            return response.json()
    except:
        pass
    return None


def format_messages(history, system_prompt: str, user_message: str):
    """æ ¼å¼åŒ–æ¶ˆæ¯ï¼Œä¿ç•™ reasoning_details ç”¨äºå¤šè½®å¯¹è¯"""
    messages = []
    if system_prompt.strip():
        messages.append({"role": "system", "content": system_prompt})
    for msg in history:
        if msg["role"] == "user":
            content = msg.get("content", "")
            if content:
                messages.append({"role": "user", "content": content})
        elif msg["role"] == "assistant":
            content = msg.get("content", "")
            if content:
                assistant_msg = {"role": "assistant", "content": content}
                # ä¿ç•™ reasoning_details ç”¨äºå¤šè½®å¯¹è¯
                if msg.get("reasoning_details"):
                    assistant_msg["reasoning_details"] = msg["reasoning_details"]
                messages.append(assistant_msg)
    messages.append({"role": "user", "content": user_message})
    return messages


def chat_stream(message: str, history: list, system_prompt: str, max_tokens: int, temperature: float, top_p: float):
    """æµå¼èŠå¤©ï¼Œè¿”å› (reasoning, content, reasoning_details) ç”Ÿæˆå™¨"""
    messages = format_messages(history, system_prompt, message)

    reasoning = ""
    content = ""
    reasoning_details = None

    try:
        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": MODEL_NAME,
            "messages": messages,
            "stream": True,
            "max_tokens": max_tokens,
            "temperature": temperature if temperature > 0 else 0.01,
            "top_p": top_p,
            "reasoning": {"enabled": True},  # å¯ç”¨æ¨ç†æ¨¡å¼
        }

        with httpx.stream("POST", f"{OPENROUTER_BASE_URL}/chat/completions", headers=headers, json=payload, timeout=120.0) as response:
            response.raise_for_status()
            for line in response.iter_lines():
                if not line or not line.startswith("data: "):
                    continue
                data_str = line[6:]
                if data_str == "[DONE]":
                    break
                try:
                    chunk = json.loads(data_str)
                    delta = chunk.get("choices", [{}])[0].get("delta", {})
                    # å¤„ç† reasoning (æµå¼æ€è€ƒå†…å®¹)
                    if delta.get("reasoning"):
                        reasoning += delta["reasoning"]
                        yield reasoning, content, reasoning_details
                    # å¤„ç† content (æµå¼å›ç­”å†…å®¹)
                    if delta.get("content"):
                        content += delta["content"]
                        yield reasoning, content, reasoning_details
                    # å¤„ç†å®Œæ•´çš„ reasoning_details (ç”¨äºå¤šè½®ä¿ç•™)
                    message_obj = chunk.get("choices", [{}])[0].get("message", {})
                    if message_obj.get("reasoning_details"):
                        reasoning_details = message_obj["reasoning_details"]
                except json.JSONDecodeError:
                    continue
        yield reasoning, content, reasoning_details

    except httpx.HTTPStatusError as e:
        yield reasoning, f"âŒ API é”™è¯¯: {e.response.status_code}", None
    except Exception as e:
        yield reasoning, f"âŒ é”™è¯¯: {str(e)}", None


def clean_thinking(text: str) -> str:
    """æ¸…ç†æ€è€ƒå†…å®¹ä¸­çš„æ ‡ç­¾"""
    if not text:
        return ""
    # ç§»é™¤ <think> æ ‡ç­¾
    text = re.sub(r'</?think>', '', text)
    return text.strip()


def render_thinking_expander(thinking_text: str, is_streaming: bool = False):
    """ä½¿ç”¨ expander æ¸²æŸ“æ€è€ƒå†…å®¹"""
    if thinking_text:
        cleaned = clean_thinking(thinking_text)
        with st.expander("ğŸ’­ æ€è€ƒè¿‡ç¨‹", expanded=is_streaming):
            st.text(cleaned)


def main():
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        system_prompt = st.text_area("ç³»ç»Ÿæç¤ºè¯", value="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚", height=80)
        max_tokens = st.slider("æœ€å¤§é•¿åº¦", 256, 131072, 4096, step=256, help="æœ€å¤§ 128k")
        temperature = st.slider("Temperature", 0.0, 1.5, 0.7, step=0.1)
        top_p = st.slider("Top-p", 0.1, 1.0, 0.9, step=0.05)

        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.divider()
        with st.expander("ğŸ“‹ æ¨¡å‹é…ç½®"):
            config = fetch_model_config()
            if config:
                st.json(config)

    # åˆå§‹åŒ– session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "pending_prompt" not in st.session_state:
        st.session_state.pending_prompt = None

    # æ ‡é¢˜
    st.title("ğŸš€ Step-3.5-Flash")

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            with st.chat_message("user"):
                st.markdown(msg["content"])
        elif msg["role"] == "assistant":
            with st.chat_message("assistant", avatar=get_assistant_avatar()):
                # æ€è€ƒå†…å®¹ç”¨ expander
                if msg.get("thinking"):
                    render_thinking_expander(msg["thinking"], is_streaming=False)
                # å›ç­”å†…å®¹ç”¨ markdown
                st.markdown(msg.get("content", ""))

    # ç¤ºä¾‹é—®é¢˜ï¼ˆæ— æ¶ˆæ¯æ—¶æ˜¾ç¤ºï¼‰
    if not st.session_state.messages:
        st.caption("ğŸ’¡ è¯•è¯•è¿™äº›é—®é¢˜ï¼š")
        examples = [
            "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "å¸®æˆ‘å†™ä¸€ä¸ª Python å¿«é€Ÿæ’åºç®—æ³•",
            "1000ä»¥å†…æœ‰å¤šå°‘ä¸ªè´¨æ•°ï¼Ÿ",
        ]
        cols = st.columns(len(examples))
        for i, example in enumerate(examples):
            if cols[i].button(example, key=f"ex_{i}", use_container_width=True):
                st.session_state.pending_prompt = example
                st.rerun()

    # è¾“å…¥æ¡†ï¼ˆå›ºå®šåœ¨åº•éƒ¨ï¼‰
    prompt = st.chat_input("è¾“å…¥æ¶ˆæ¯...")

    # å¤„ç† pending_promptï¼ˆæ¥è‡ªç¤ºä¾‹æŒ‰é’®ï¼‰
    if st.session_state.pending_prompt:
        prompt = st.session_state.pending_prompt
        st.session_state.pending_prompt = None

    if prompt:
        # æ·»åŠ å¹¶æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # åŠ©æ‰‹å›å¤
        with st.chat_message("assistant", avatar=get_assistant_avatar()):
            # æ€è€ƒå†…å®¹å ä½ç¬¦
            thinking_placeholder = st.empty()
            # å›ç­”å†…å®¹å ä½ç¬¦
            answer_placeholder = st.empty()

            full_response = ""
            full_thinking = ""
            full_reasoning_details = None

            for thinking, response, reasoning_details in chat_stream(
                prompt,
                st.session_state.messages[:-1],
                system_prompt,
                max_tokens,
                temperature,
                top_p,
            ):
                full_thinking = thinking
                full_response = response if response else "â–Œ"
                if reasoning_details:
                    full_reasoning_details = reasoning_details

                # æ›´æ–°æ€è€ƒå†…å®¹
                if full_thinking:
                    with thinking_placeholder.container():
                        render_thinking_expander(full_thinking, is_streaming=True)

                # æ›´æ–°å›ç­”å†…å®¹
                answer_placeholder.markdown(full_response)

            # ä¿å­˜æ¶ˆæ¯ï¼ˆåŒ…å« reasoning_details ç”¨äºå¤šè½®å¯¹è¯ï¼‰
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "thinking": full_thinking,
                "reasoning_details": full_reasoning_details,
            })
            st.rerun()


if __name__ == "__main__":
    main()
